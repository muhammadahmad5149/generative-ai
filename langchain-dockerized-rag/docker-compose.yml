version: '3.10'

services:
  llm-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-chatbot-app-simple
    volumes:
      - .:/app:cached
    depends_on:
      - ollama
      - chroma
    stdin_open: true
    tty: true
    restart: unless-stopped
    networks:
      - llm-network
    env_file:
      - .env
    ports:
      - "${API_PORT}:${API_PORT}"

  ollama:
    image: ollama/ollama
    container_name: ollama-simple
    ports:
      - "11435:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - llm-network
    restart: unless-stopped
    entrypoint: ["/bin/bash", "-c", "\
      ollama serve & \
      sleep 5 && \
      ollama pull mxbai-embed-large && \
      ollama pull llama3.2 && \
      wait"]

  chroma:
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chroma-db-simple
    ports:
      - "9012:9000"
    volumes:
      - chroma-data:/chroma
    networks:
      - llm-network
    restart: unless-stopped

networks:
  llm-network:
    driver: bridge

volumes:
  ollama-data:
  chroma-data:
