{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c1da88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from config import settings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8217b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages:Annotated[list,add_messages]\n",
    "llm = OllamaLLM(model=settings.OLLAMA_MODEL, base_url=settings.OLLAMA_BASE_URL)\n",
    "embeddings = OllamaEmbeddings(model=settings.EMBEDDING_MODEL, base_url=settings.OLLAMA_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654adb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "loader = TextLoader(settings.DATA_FILE)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "documents = loader.load()\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Setup Chroma as vectorstore\n",
    "# persist_directory = \"chroma_db\"  # or any other persistent directory\n",
    "\n",
    "persist_directory = os.path.abspath(\"chroma_db\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name= 'rag-chroma',\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "\n",
    "# (Optional) Persist to disk\n",
    "vectorstore.persist()\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f330d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "custom_prompt =  ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a precise accounting information assistant for GL.\n",
    "Use only the provided financial documentation to answer questions factually.\n",
    "If information isn't available, respond professionally with:\n",
    "\"I don't have that specific information in our records - please contact our accounting team for assistance.\"\n",
    "\n",
    "Answer requirements:\n",
    "1. Provide only accounting/finance facts from the context\n",
    "2. Never add conversational fluff or invitations to ask more\n",
    "3. Quote exact figures/dates when available\n",
    "4. Reference relevant forms/regulations where applicable\n",
    "5. Keep answers under 3 sentences unless technical details require more\n",
    "\n",
    "For unrelated questions:\n",
    "\"That question falls outside our accounting scope - please contact client services.\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Concise accounting answer:\"\"\")\n",
    "\n",
    "def chatbot(state: State) -> State:\n",
    "    last_user_msg = state[\"messages\"][-1].content\n",
    "    context_docs = state[\"messages\"][:-1]  # earlier messages are docs\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    formatted_prompt = custom_prompt.invoke({\"context\": context, \"question\": last_user_msg})\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    return {\"messages\": state[\"messages\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc69adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and configures the state graph for handling queries and generating answers.\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "# workflow.add_node(\"retrieve\", retriever)\n",
    "workflow.add_node(\"generate\", chatbot)\n",
    "# workflow.add_edge(START, \"retrieve\")\n",
    "# workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "# workflow.compile()\n",
    "graph=workflow.compile()\n",
    "response=graph.invoke({\"messages\": \"Hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27451fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='8c5432aa-d825-4feb-980a-0a594dd1fe7b')]}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfc066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the graph\n",
    "from IPython.display import Image,display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bc1f2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chain\n",
    "# qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "#     llm=llm,\n",
    "#     retriever=retriever,\n",
    "#     # memory=memory,\n",
    "#     combine_docs_chain_kwargs={\"prompt\": custom_prompt.partial(firm_name=\"GL\")},\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "# result = qa_chain.invoke({\"messages\": [HumanMessage(content=\"What is GL software\")]})\n",
    "# for content in result[\"messages\"]:\n",
    "#     print(content)\n",
    "# print(result[\"messages\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
