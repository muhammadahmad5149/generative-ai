{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e5d3ed",
   "metadata": {},
   "source": [
    "### Basic LangGraph Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d78fdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START, StateGraph,END\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from config import settings\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b78f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "llm = OllamaLLM(model=\"mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee7a2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='mistral')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165a4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chatbot_node(state: State) -> State:\n",
    "#     \"\"\" The main chatbot node that processes the user input and returns the response \"\"\"\n",
    "#     # print(f\"Processing {state['messages']} messages\")\n",
    "#     response =  llm.invoke(state['messages'])\n",
    "#     print(f\"Response: {response}\")\n",
    "#     return {\"messages\": state['messages']}\n",
    "\n",
    "# print(\"Chatbot node function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e712766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created\n"
     ]
    }
   ],
   "source": [
    "# --- Create the retriever BEFORE the graph ---\n",
    "def get_retriever():\n",
    "    embeddings = OllamaEmbeddings(model=settings.EMBEDDING_MODEL, base_url=settings.OLLAMA_BASE_URL)\n",
    "    loader = TextLoader(settings.DATA_FILE)\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    documents = loader.load()\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    persist_directory = os.path.abspath(\"chroma_db\")\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name= 'rag-chroma',\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "\n",
    "    print(\"Retriever created\")\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "retriever = get_retriever()  # Create once and reuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd36b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_builder = StateGraph(State)\n",
    "# graph_builder.add_node(\"retrieve\", split_documents)\n",
    "# graph_builder.add_node(\"chatbot\", chatbot_node)\n",
    "# graph_builder.add_edge(START, \"retrieve\")\n",
    "# graph_builder.add_edge(\"retrieve\", \"chatbot\")\n",
    "# graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# graph = graph_builder.compile()\n",
    "\n",
    "# print(\"Graph structure created\")\n",
    "# print(\"Graph compiled successfully\")\n",
    "# graph.invoke({\"messages\": [HumanMessage(content=\"How secure is financial data in GL?\")]})\n",
    "# # graph.invoke({\"messages\": \"How secure is financial data in GL?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26146e16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m img= \u001b[43mgraph\u001b[49m.get_graph(xray=\u001b[38;5;28;01mTrue\u001b[39;00m).draw_mermaid_png()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgraph.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     f.write(img)\n",
      "\u001b[31mNameError\u001b[39m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "img= graph.get_graph(xray=True).draw_mermaid_png()\n",
    "with open(\"graph.png\", \"wb\") as f:\n",
    "    f.write(img)\n",
    "from IPython.display import Image\n",
    "display(Image(\"graph.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97609cd3",
   "metadata": {},
   "source": [
    "### TESTING THE CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_chatbot(message: str):\n",
    "#     \"\"\"Helper function to test the chatbot\"\"\"\n",
    "#     initial_state={\"messages\": [HumanMessage(content=message)]}\n",
    "#     print(\"svsdv\",initial_state)\n",
    "#     response = graph.invoke(initial_state)\n",
    "#     ai_response = response[\"messages\"][-1].content\n",
    "#     print(\"AI:\", ai_response)\n",
    "#     # print(\"Response:\", response)\n",
    "#     return response\n",
    "# test_cases = [\n",
    "#     \"Hello my name is Ahmad\",\n",
    "#     \"Do you know who I am?\",\n",
    "# ]\n",
    "# for test_case in test_cases:\n",
    "#     test_chatbot(test_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d165a2",
   "metadata": {},
   "source": [
    "### ADDING MEMORY TO THE CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2380107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = MemorySaver()\n",
    "\n",
    "# graph_with_memory = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# # print(\"Memory added to the graph\")\n",
    "\n",
    "# def chat_with_meomry(message: str, thread_id:str):\n",
    "#     \"\"\" Chat function with memory \"\"\"\n",
    "#     print(\"User:\", message)\n",
    "\n",
    "#     config = {\n",
    "#         \"configurable\": {\n",
    "#             \"thread_id\": thread_id\n",
    "#         }\n",
    "#     }\n",
    "#     initial_state = {\"messages\": [HumanMessage(content=message)]}\n",
    "#     result = graph_with_memory.invoke(initial_state, config)\n",
    "#     ai_response = result[\"messages\"][-1].content\n",
    "#     # print(\"AI:\", ai_response)\n",
    "#     # print(\"Memory:\",result)\n",
    "#     return result\n",
    "\n",
    "# chat_with_meomry(\"Hi my name is Ahmad\", \"thread-1\")\n",
    "# chat_with_meomry(\"What is my name?\", \"thread-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd579d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    context: str  # Or list[str] if multiple docs\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_node(state: State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    docs = retriever.invoke(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "\n",
    "def chatbot_node(state: State) -> State:\n",
    "    prompt = f\"\"\"\n",
    "You are a precise accounting information assistant for GL.\n",
    "Use only the provided financial documentation to answer questions factually.\n",
    "If information isn't available, respond professionally with:\n",
    "\"I don't have that specific information in our records - please contact our accounting team for assistance.\"\n",
    "\n",
    "Answer requirements:\n",
    "1. Provide only accounting/finance facts from the context\n",
    "2. Never add conversational fluff or invitations to ask more\n",
    "3. Quote exact figures/dates when available\n",
    "4. Reference relevant forms/regulations where applicable\n",
    "5. Keep answers under 3 sentences unless technical details require more\n",
    "\n",
    "For unrelated questions:\n",
    "\"That question falls outside our accounting scope - please contact client services.\"\n",
    "\n",
    "Context: {state['context']}\n",
    "Question: {state['messages'][-1].content}\n",
    "Concise accounting answer:\"\"\"\n",
    "\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [response],\n",
    "        \"context\": state[\"context\"]\n",
    "    }\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve\", retrieve_node)\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph_with_memory = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ee39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GL refers to General Ledger, which is an accounting tool designed to manage and track all financial transactions, automate journal entries, generate reports, and ensure compliance with accounting standards.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"1\"\n",
    "        }\n",
    "    }\n",
    "output= graph_with_memory.invoke({\"messages\": [HumanMessage(content=\"What is GL?\")]}, config=config)\n",
    "print(output[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "907a8c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The consistency principle states that an adopted accounting method should be used consistently over time for accurate comparison, and the materiality concept says only significant items affecting financial decisions should be reported, allowing for practical precision in records.\n"
     ]
    }
   ],
   "source": [
    "output= graph_with_memory.invoke({\"messages\": [HumanMessage(content=\"What did I just tell you? \")]}, config=config)\n",
    "print(output[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4ec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Muhammad Ahmad! In the provided context, a ledger is a book or digital record where all journal entries are posted to individual accounts. It helps track the balance of each account and prepares data for the trial balance and financial statements. A trial balance lists all the ledger account balances at a specific point in time to verify the arithmetic accuracy of bookkeeping by ensuring that total debits equal total credits. I hope this answers your question! If you have any other questions, feel free to ask!\n",
      " You just told me that a credit note is issued when goods are returned or overcharged, reducing the amount owed by the customer and adjusting the accounts receivable balance. A debit note is issued to request a credit for returned goods or billing adjustments. It informs the seller about the amount being claimed back.\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "output1 = graph_with_memory.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, my name is Muhammad Ahmad.\")]},\n",
    "    config=config\n",
    ")\n",
    "print(output1[\"messages\"][-1].content)\n",
    "\n",
    "# Second message — memory will kick in!\n",
    "output2 = graph_with_memory.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What did I just tell you?\")]},\n",
    "    config=config\n",
    ")\n",
    "print(output2[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
